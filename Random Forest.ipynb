{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees and ansmbles (on the example of Random Forest)\n",
    "=========================\n",
    "***\n",
    "\n",
    "### How do we handle data when the number of variables is very high? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What Decision tree is?</b>\n",
    "\n",
    "A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"titanic_survivors.png\" />\n",
    "\n",
    "This graphic from Wikipedia [1] represents intrinsic structure of the \"Titanic\" dataset, data on the survivors of the Titanic disater.\n",
    "\n",
    "The information in a decision tree format. The numbers next to each node are the probability of survival and the % of the observations that were assigned to (classified as) the category represented by this node. Each left branch corresponds to a \"yes\" answer, the right one a \"no\".  Each green node represents \"survived\", each red one \"did not survive\".\n",
    "\n",
    "The number of spouses or siblings aboard is recorded as \"sibsp\".\n",
    "\n",
    "As is well known you had a much smaller chance of surviving if you were male and in the less expensive berths. You had a much greater chance of surviving if you were an infant or female and in the most expensive berths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Often a problem that needs to be tackled is so large or complex that we need a group of experts not just a single one to tackle it.  Linux, for example, is such a complex system that building it took hundreds of experts.\n",
    "\n",
    "What if we could harness the decision-making power and the subject matter expertise of many experts and use it in Data Science?  There is such a technique called Random Forests which uses collective decision-making to improve on the outcome possible with a single decision maker.  In this approach each software \"expert\" uses a tree-based algorithm to do their bit and then a collection of such trees is used to compute or evolve a model that is better than the output of any one expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree learning uses a decision tree as a predictive model which maps observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of \"goodness\" for decision trees\n",
    "The science of Decision Trees quantifies all this using measures called Information Gain and Entropy.  Essentially we want to have just the right amount of splits so that we don't keep splitting a group once we have the \"best\" split.  So how do we know when one way to split is better than another.  Obviously if one split leads to a clean partition into 'admit' vs. 'reject' then it's good.\n",
    "\n",
    "This kind of variation in a set indicates a higher \"entropy\" while a set with all identical members has very low or zero \"entropy\". So, when we split a set we want the halves to be more distinct from each other and the members in the group to be more like each other -- i.e. we want entropy to go down as we keep splitting.  So if we use an approach that doesn't reduce the entropy by much it is probably not a good attribute or a good value to split on.  \n",
    "\n",
    "If we take a Decision Tree that has been created and we reverse the process, then when we combine two nodes, we will increase entropy or variation as groups get combined,  The gain in entropy is called Information Gain.  So the best splits are those which give the best Information Gain when reversed.\n",
    "\n",
    "This is all very loose but has a strong mathematical foundation that is used to construct the  modeling software that creates such \"decision tree\" models.\n",
    "\n",
    "When given a set of samples with many attributes, a decision tree model will identify the attributes that are best to split on and the values of those attributes that we should use to do the splitting.  It will then print out a number of parameters, including number of attributes used to split, which ones, and Information Gain,... etc.\n",
    "\n",
    "So how does modeling software decide the best tree?  It tries every one and compares Information Gain for each and then picks the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the Forest\n",
    "<b>In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.</b>\n",
    "\n",
    "Decision Trees present a simple clean conceptual model to understand classification by an iterative procedure. However, in practice, a single Decision Tree is not very useful for real world problems involving a large number of variables and moderate to large sized data.  \n",
    "For this we need heavy artillery. A group of \"experts\" constituting a Random Forest.  \n",
    "\n",
    "But what is an \"expert\" in this scenario?\n",
    "\n",
    "If we consider our \"expert\" to have in their head a decision tree modeler and we assemble say 100 such experts, then, loosely speaking, we have the makings of a Random Forest. We want a collection of experts to decide our result, expecting that the result will be much better than a single one.  So we will need some way to decide how to collate and sort through the \"opinions\".  \n",
    "\n",
    "If you recall the Olympic Gymnastic competitions or diving competitions where a panel of judges scores a participant, you might remember that the top and bottom scores are dropped and the rest are averaged.  A Random Forest algorithm uses such techniques to eliminate some of the opinions but might randomly drop some percentage and then rerun the \"competition\", doing this each time and then averaging the result after say 100 such trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use such complicated techniques?\n",
    "Well for one, they are more accurate, as mathematicaly provable.  But also because, when we have 10s or 100s of attributes Random Forests are able to surface the most significant ones and use these in their modeling without any extra effort on our part.  So what's the catch?  This comes at some computational cost so our model may run for many minutes instead of a few seconds even with a few thousand samples, since orders of magnitude more calculations are being done.  However there are many more benefits for this one cost.  \n",
    "Random Forests are much more tolerant of missing values, bad data, and outliers, and can handle mixed data types, numerical and categorical.\n",
    "\n",
    "We will explore a rich data set generated from the accelerometer and gyroscope of mobile phones, and use it to understand various activities of the user - such as sitting, standing, walking etc., based on particular combinations of the data attributes.  Our data has more than 500 such attributes and the data is also messy and rich so this is a good candiadte for combining domain knowledge with the power of Random Forests in the exploration and analysis to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "    + The ability to effectively handle data with a large number of attributes and classes\n",
    "    + Insensitive to scale\n",
    "    + Equally well handle both continuous and discrete attributes\n",
    "    + There are methods for estimating the significance of individual features in the model\n",
    "    + The internal ability of the model to generalize (test out-of-bag)\n",
    "    - The algorithm is prone to retrain some tasks, especially on noisy data\n",
    "    - The large size of the resulting models. Requires O(NK) memory for storing a model where K - number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use a large and messy data set from a familiar source object and then prepare it for analysis using Random Forests.\n",
    "Why do we want to use Random Forests? This will become clear very shortly.\n",
    "\n",
    "We will use a data set of mobile phone accelerometer and gyroscope readings to create a predictive model. The data set is found in R Data form on Amazon S3 and raw form at the UCI Repository The data set readings encode data on mobile phone orientation and motion of the wearer of the phone.\n",
    "\n",
    "The subject is known to be doing one of six activities - sitting, standing, lying down, walking, walking up, and walking down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: \"Charis SIL\", Palatino, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 120%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_dict = {'laying':1, 'sitting':2, 'standing':3, 'walk':4, 'walkup':5, 'walkdown':6} \n",
    "\n",
    "def remap_col(df,colname, mapping=None):\n",
    "  if not mapping:\n",
    "    global map_dict\n",
    "    mapping = map_dict.copy()\n",
    "    \n",
    "  df[colname] = df[colname].map(lambda x: mapping[x]) \n",
    "  return df\n",
    "\n",
    "def intersect(a,b):\n",
    "  \"\"\"Intersect two lists\"\"\"\n",
    "  return [val for val in a if val in b]\n",
    "\n",
    "def errormeasures(orig, pred, activity):\n",
    "  \"\"\"docstring for errormeasures\"\"\"\n",
    "  n = len(orig)\n",
    "  origtrue = []\n",
    "  origfalse = []\n",
    "  predtrue = []\n",
    "  predfalse = []\n",
    "  \n",
    "  for i in range(1,n+1):\n",
    "    if(orig[i] == act):\n",
    "      origtrue.append(i)          \n",
    "    else: \n",
    "      origfalse.append(i)          \n",
    "\n",
    "    if(pred[i] == act):\n",
    "      predtrue.append(i)          \n",
    "    else:\n",
    "      predfalse.append(i)\n",
    "      \n",
    "      \n",
    "  # compute the members of the quadrant\n",
    "  truepos = len(intersect(origtrue, predtrue))\n",
    "  trueneg = len(intersect(origfalse, predfalse))\n",
    "  falsepos = len(intersect(origfalse, predtrue))\n",
    "  falseneg = len(intersect(origtrue, predfalse))\n",
    "      \n",
    "                    \n",
    "  # compute the 4 measures below\n",
    "  #\n",
    "  # PosPred:    Positive Predictive Value\n",
    "  # NegPred:    Negative Predictive Value\n",
    "  # Sens:       Sensitivity\n",
    "  # Spec:       Specificity\n",
    "    \n",
    "  pospred = truepos/(truepos + falsepos)\n",
    "  negpred = trueneg/(trueneg + falseneg)\n",
    "  sens    = truepos/(truepos + falseneg)\n",
    "  spec    = trueneg/(trueneg + falsepos)\n",
    "  \n",
    "  return [truepos,trueneg,falsepos,falseneg,pospred,negpred,sens,spec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "# We pull in the training, validation and test sets created according to the scheme described\n",
    "# in the data exploration lesson.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "samtrain = pd.read_csv('datasets/samsung/samtrain.csv')\n",
    "samval = pd.read_csv('datasets/samsung/samval.csv')\n",
    "samtest = pd.read_csv('datasets/samsung/samtest.csv')\n",
    "\n",
    "import randomforests as rf\n",
    "samtrain = rf.remap_col(samtrain,'activity')\n",
    "samval = rf.remap_col(samval,'activity')\n",
    "samtest = rf.remap_col(samtest,'activity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble as sk\n",
    "\n",
    "rfc = sk.RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "\n",
    "train_data = samtrain[samtrain.columns[1:-2]]\n",
    "train_truth = samtrain['activity']\n",
    "model = rfc.fit(train_data, train_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97946768060836498"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the OOB (out of band) score which is an estimate of accuracy of our model.\n",
    "# the out-of-bag estimate is as accurate as using a test set of the same size as the training set\n",
    "rfc.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.044190100358059189, 'Unnamed: 0'),\n",
       " (0.053268203404881885, 'tAccMean'),\n",
       " (0.04239672375472528, 'tAccStd'),\n",
       " (0.045004926716972982, 'tJerkMean'),\n",
       " (0.048653796741901095, 'tGyroJerkMagSD'),\n",
       " (0.051780842823361213, 'fAccMean'),\n",
       " (0.042449586534545565, 'fJerkSD'),\n",
       " (0.040054703486826361, 'fJerkMeanFreq'),\n",
       " (0.13888417140937864, 'angleGyroJerkGravity'),\n",
       " (0.16565857065828607, 'angleXGravity'),\n",
       " (0.046362659634145258, 'angleYGravity')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use \"feature importance\" scores to see what the top 10 important features are\n",
    "fi = enumerate(rfc.feature_importances_)\n",
    "cols = samtrain.columns\n",
    "[(value,cols[i]) for (i,value) in fi if value > 0.04]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas data frame adds a spurious unknown column in 0 position hence starting at col 1\n",
    "# not using subject column, activity ie target is in last columns hence -2 i.e dropping last 2 cols\n",
    "\n",
    "val_data = samval[samval.columns[1:-2]]\n",
    "val_truth = samval['activity']\n",
    "val_pred = rfc.predict(val_data)\n",
    "\n",
    "test_data = samtest[samtest.columns[1:-2]]\n",
    "test_truth = samtest['activity']\n",
    "test_pred = rfc.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy score for validation set = 0.833333\n",
      "mean accuracy score for test set = 0.893603\n"
     ]
    }
   ],
   "source": [
    "print(\"mean accuracy score for validation set = %f\" %(rfc.score(val_data, val_truth)))\n",
    "print(\"mean accuracy score for test set = %f\" %(rfc.score(test_data, test_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the confusion matrix to see how observations were misclassified as other activities\n",
    "# See [5]\n",
    "import sklearn.metrics as skm\n",
    "test_cm = skm.confusion_matrix(test_truth,test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEDCAYAAAAY88llAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffb5ae9ddd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "pl.matshow(test_cm)\n",
    "pl.title('Confusion matrix for test data\\n'\n",
    "         + '                               ')\n",
    "pl.colorbar()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.893603\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy = %f\" %(skm.accuracy_score(test_truth,test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.896920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darth/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "print(\"Precision = %f\" %(skm.precision_score(test_truth,test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = 0.894181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darth/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "# F1 Score\n",
    "print(\"F1 score = %f\" %(skm.f1_score(test_truth,test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "https://en.wikipedia.org/wiki/Random_forest\n",
    "http://learnds.com/\n",
    "http://nbviewer.jupyter.org/github/agconti/kaggle-titanic/blob/master/Titanic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
